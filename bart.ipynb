{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO/GoNLqgP1TNzTupTJIEvX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JdeGraftJohnson/Python-R-Portfolio/blob/main/bart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "q4Casu4jAv2K",
        "outputId": "388c5cd9-5f09-464b-c207-8d365f20e5c7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "partially initialized module 'torch' has no attribute 'fx' (most likely due to a circular import)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-9325df979a41>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2220\u001b[0m \u001b[0;31m# quantization depends on torch.fx and torch.ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2221\u001b[0m \u001b[0;31m# Import quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2222\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquantization\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mquantization\u001b[0m  \u001b[0;31m# usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2224\u001b[0m \u001b[0;31m# Import the quasi random sampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/quantization/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfuser_method_mappings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mobserver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mqconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mquant_type\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mquantization_mappings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/quantization/qconfig.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mhere\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \"\"\"\n\u001b[0;32m----> 9\u001b[0;31m from torch.ao.quantization.qconfig import (\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0m_add_module_to_qconfig_obs_ctr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0m_assert_valid_qconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfuser_method_mappings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mobserver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m from .pt2e._numeric_debugger import (  # noqa: F401\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mcompare_results\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mCUSTOM_KEY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/pt2e/_numeric_debugger.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_sqnr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpt2e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_control_flow_submodules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExportedProgram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraphModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/pt2e/graph_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from torch.fx.passes.utils.source_matcher_utils import (\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mcheck_subgraphs_connected\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mget_source_partitions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/fx/passes/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from . import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mgraph_drawer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgraph_manipulation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnet_min_base\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moperator_support\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/fx/passes/graph_drawer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_format_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_get_qualified_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperator_schemas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalize_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_prop\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorMetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/fx/passes/shape_prop.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcompatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_backward_compatible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mShapeProp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterpreter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \"\"\"\n\u001b[1;32m     87\u001b[0m     \u001b[0mExecute\u001b[0m \u001b[0man\u001b[0m \u001b[0mFX\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mNode\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torch' has no attribute 'fx' (most likely due to a circular import)"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Financial Prediction Model using PyMC-BART on Google Colab\n",
        "\n",
        "This script implements a hybrid model that combines neural network embeddings\n",
        "with Bayesian Additive Regression Trees for financial time series forecasting\n",
        "with uncertainty quantification.\n",
        "\n",
        "Optimized for Google Colab GPU environment.\n",
        "\"\"\"\n",
        "\n",
        "# Required packages are already installed according to your output\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import re\n",
        "import glob\n",
        "from typing import Dict, List, Optional, Tuple, Union\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from typing import Optional, List, Dict, Union\n",
        "import pymc as pm\n",
        "import pymc_bart as pmb\n",
        "import arviz as az\n",
        "import matplotlib.pyplot as plt\n",
        "from pymc_bart.split_rules import SplitRule\n",
        "import pymc_bart as pmb\n",
        "\n",
        "# Check for available hardware acceleration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device for PyTorch: {device}\")\n",
        "print(\"Using device for PyMC: CPU (default)\")\n",
        "\n",
        "# Setup for data storage - try Google Drive first, with fallback to local Colab storage\n",
        "USE_DRIVE = False\n",
        "DRIVE_PATH = '/content/drive/MyDrive/financial_bart_model'\n",
        "LOCAL_PATH = '/content/financial_bart_model'\n",
        "\n",
        "try:\n",
        "    from google.colab import drive, files\n",
        "    # Try to mount Google Drive\n",
        "    drive.mount('/content/drive')\n",
        "    # Create directory for saving models and results if it doesn't exist\n",
        "    os.makedirs(DRIVE_PATH, exist_ok=True)\n",
        "    print(f\"Successfully mounted Google Drive. Using path: {DRIVE_PATH}\")\n",
        "    USE_DRIVE = True\n",
        "except Exception as e:\n",
        "    print(f\"Could not mount Google Drive: {str(e)}\")\n",
        "    print(f\"Using local Colab storage at: {LOCAL_PATH}\")\n",
        "    os.makedirs(LOCAL_PATH, exist_ok=True)\n",
        "\n",
        "# Set the storage path based on Drive availability\n",
        "STORAGE_PATH = DRIVE_PATH if USE_DRIVE else LOCAL_PATH\n",
        "\n",
        "# Constants for model configuration\n",
        "EMBEDDING_DIM = 768  # Standard dimension for all embeddings\n",
        "FUSION_OUTPUT_DIM = 128  # Output dimension after fusion\n",
        "\n",
        "class EmbeddingProcessor(nn.Module):\n",
        "    \"\"\"\n",
        "    Processes embeddings from multiple financial data sources\n",
        "    and fuses them for downstream Bayesian modeling\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                embedding_dim=EMBEDDING_DIM,\n",
        "                output_dim=FUSION_OUTPUT_DIM):\n",
        "        super().__init__()\n",
        "\n",
        "        # All projections use the same embedding dimension for consistency\n",
        "        self.stock_projection = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, output_dim)\n",
        "        )\n",
        "\n",
        "        self.sentiment_projection = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, output_dim)\n",
        "        )\n",
        "\n",
        "        # Feature fusion layer\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(2 * output_dim, output_dim),\n",
        "            nn.LayerNorm(output_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Move model to appropriate device\n",
        "        self.to(device)\n",
        "\n",
        "    def extract_features(self, embeds, projection_layer):\n",
        "        \"\"\"Extract features using the specified projection layer\"\"\"\n",
        "        # Handle both 2D and 3D inputs consistently\n",
        "        if len(embeds.shape) == 3:\n",
        "            embeds = torch.mean(embeds, dim=1)\n",
        "        return projection_layer(embeds)\n",
        "\n",
        "    def forward(self, stock_embeds, sentiment_embeds):\n",
        "        \"\"\"\n",
        "        Forward pass through the embedding processor\n",
        "\n",
        "        Args:\n",
        "            stock_embeds: Stock data embeddings [batch_size, embedding_dim]\n",
        "            sentiment_embeds: Sentiment data embeddings [batch_size, embedding_dim]\n",
        "\n",
        "        Returns:\n",
        "            Processed features for Bayesian modeling [batch_size, output_dim]\n",
        "        \"\"\"\n",
        "        # Move inputs to the correct device\n",
        "        stock_embeds = stock_embeds.to(device)\n",
        "        sentiment_embeds = sentiment_embeds.to(device)\n",
        "\n",
        "        # Project each data source\n",
        "        stock_features = self.extract_features(stock_embeds, self.stock_projection)\n",
        "        sentiment_features = self.extract_features(sentiment_embeds, self.sentiment_projection)\n",
        "\n",
        "        # Concatenate features\n",
        "        combined = torch.cat([stock_features, sentiment_features], dim=1)\n",
        "\n",
        "        # Fuse features\n",
        "        fused = self.fusion(combined)\n",
        "\n",
        "        return fused\n",
        "\n",
        "\n",
        "class PyMcBartPredictionHead:\n",
        "    \"\"\"BART model with PyMC-BART prediction head for financial forecasting with uncertainty.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_trees=50,\n",
        "        alpha=0.90,  # Slightly reduced from 0.95 to allow more flexibility\n",
        "        beta=1.5,    # Slightly reduced from 2.0 to allow more flexibility\n",
        "        num_classes=None,  # Set for classification tasks\n",
        "        response_type=\"continuous\"  # 'continuous', 'binary', or 'categorical'\n",
        "    ):\n",
        "        self.n_trees = n_trees\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.num_classes = num_classes\n",
        "        self.response_type = response_type\n",
        "        self.model = None\n",
        "        self.idata = None  # To store inference data\n",
        "        self.is_fitted = False\n",
        "\n",
        "    def _build_model(self, X, y=None):\n",
        "        \"\"\"Build the PyMC-BART model.\"\"\"\n",
        "        with pm.Model() as model:\n",
        "            # Define BART model based on response type\n",
        "            if self.response_type == \"continuous\":\n",
        "                # Regression model without jitter (which caused the error)\n",
        "                μ = pmb.BART(\n",
        "                    \"μ\",\n",
        "                    X,\n",
        "                    y,\n",
        "                    m=self.n_trees,\n",
        "                    alpha=self.alpha,\n",
        "                    beta=self.beta\n",
        "                )\n",
        "\n",
        "                # Add likelihood\n",
        "                σ = pm.HalfNormal(\"σ\", 1.0)\n",
        "                y_pred = pm.Normal(\"y_pred\", mu=μ, sigma=σ, observed=y)\n",
        "\n",
        "            elif self.response_type == \"binary\":\n",
        "                # Binary classification model\n",
        "                μ = pmb.BART(\n",
        "                    \"μ\",\n",
        "                    X,\n",
        "                    y,\n",
        "                    m=self.n_trees,\n",
        "                    alpha=self.alpha,\n",
        "                    beta=self.beta\n",
        "                )\n",
        "\n",
        "                # Add sigmoid link and likelihood\n",
        "                p = pm.Deterministic(\"p\", pm.math.sigmoid(μ))\n",
        "                y_pred = pm.Bernoulli(\"y_pred\", p=p, observed=y)\n",
        "\n",
        "            elif self.response_type == \"categorical\":\n",
        "                # Ensure we have the proper dimensions for multiclass classification\n",
        "                if self.num_classes is None:\n",
        "                    raise ValueError(\"For categorical response, num_classes must be specified\")\n",
        "\n",
        "                # Set up coordinates for multiple outputs\n",
        "                coords = {\"classes\": range(self.num_classes), \"obs_id\": range(len(X))}\n",
        "\n",
        "                with pm.Model(coords=coords) as model:\n",
        "                    # For categorical, we need one BART model per class\n",
        "                    μ = pmb.BART(\n",
        "                        \"μ\",\n",
        "                        X,\n",
        "                        y,\n",
        "                        m=self.n_trees,\n",
        "                        alpha=self.alpha,\n",
        "                        beta=self.beta,\n",
        "                        dims=[\"classes\", \"obs_id\"]\n",
        "                    )\n",
        "\n",
        "                    # Apply softmax to get probabilities\n",
        "                    θ = pm.Deterministic(\"θ\", pm.math.softmax(μ, axis=0))\n",
        "\n",
        "                    # Categorical likelihood\n",
        "                    y_pred = pm.Categorical(\"y_pred\", p=θ.T, observed=y)\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported response type: {self.response_type}\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        X,\n",
        "        y,\n",
        "        chains=4,  # Increased to 4 chains for better uncertainty estimates\n",
        "        tune=1000,\n",
        "        draws=1000,\n",
        "        cores=2,  # Use multiple cores on Colab\n",
        "        random_seed=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Fit the PyMC-BART model.\n",
        "\n",
        "        Args:\n",
        "            X: Feature matrix\n",
        "            y: Target values\n",
        "            chains: Number of MCMC chains\n",
        "            tune: Number of tuning samples\n",
        "            draws: Number of posterior samples\n",
        "            cores: Number of cores to use\n",
        "            random_seed: Random seed for reproducibility\n",
        "\n",
        "        Returns:\n",
        "            self\n",
        "        \"\"\"\n",
        "        # Build the model\n",
        "        self.model = self._build_model(X, y)\n",
        "\n",
        "        # Sample from the posterior\n",
        "        with self.model:\n",
        "            self.idata = pm.sample(\n",
        "                draws=draws,\n",
        "                tune=tune,\n",
        "                chains=chains,\n",
        "                cores=cores,\n",
        "                random_seed=random_seed,\n",
        "                compute_convergence_checks=True\n",
        "            )\n",
        "\n",
        "            # Get posterior predictive samples\n",
        "            pm.sample_posterior_predictive(\n",
        "                self.idata,\n",
        "                extend_inferencedata=True\n",
        "            )\n",
        "\n",
        "        self.is_fitted = True\n",
        "        return self\n",
        "\n",
        "    def predict(self, X, return_std=False, samples=100):\n",
        "        \"\"\"\n",
        "        Make predictions with uncertainty estimates.\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise RuntimeError(\"Model must be fitted before making predictions\")\n",
        "\n",
        "        # Get posterior samples for μ\n",
        "        μ_samples = self.idata.posterior[\"μ\"].values\n",
        "\n",
        "        # Debug information\n",
        "        print(\"Original μ_samples shape:\", μ_samples.shape)\n",
        "        print(\"Sample values from μ_samples:\", μ_samples[0, 0, :5])  # Print first 5 values from first chain/draw\n",
        "\n",
        "        # Properly reshape samples from multiple chains\n",
        "        # Reshape to (chains*draws, observations)\n",
        "        μ_samples = μ_samples.reshape(-1, μ_samples.shape[-1])\n",
        "\n",
        "        # Take a subset of samples if needed\n",
        "        if samples < μ_samples.shape[0]:\n",
        "            μ_samples = μ_samples[:samples]\n",
        "\n",
        "        print(\"Reshaped μ_samples shape:\", μ_samples.shape)\n",
        "\n",
        "        # For binary classification, apply sigmoid\n",
        "        if self.response_type == \"binary\":\n",
        "            μ_samples = 1.0 / (1.0 + np.exp(-μ_samples))\n",
        "\n",
        "        # For categorical classification, apply softmax\n",
        "        elif self.response_type == \"categorical\":\n",
        "            μ_samples = np.transpose(μ_samples, (0, 2, 1))\n",
        "            exp_scores = np.exp(μ_samples)\n",
        "            μ_samples = exp_scores / np.sum(exp_scores, axis=2, keepdims=True)\n",
        "\n",
        "        # Check for variation across samples\n",
        "        sample_variation = np.var(μ_samples, axis=0)\n",
        "        print(\"Variance across samples (first 5):\", sample_variation[:5])\n",
        "        print(\"Min/max/mean variance:\", np.min(sample_variation), np.max(sample_variation), np.mean(sample_variation))\n",
        "\n",
        "        # Compute mean predictions\n",
        "        predictions = np.mean(μ_samples, axis=0)\n",
        "\n",
        "        if return_std:\n",
        "            # Compute standard deviation of predictions with small epsilon to avoid zeros\n",
        "            std_predictions = np.std(μ_samples, axis=0)\n",
        "            print(\"Raw std calculations (first 5):\", std_predictions[:5])\n",
        "\n",
        "            # Ensure uncertainties aren't exactly zero by adding a small epsilon\n",
        "            epsilon = 1e-8\n",
        "            std_predictions = np.maximum(std_predictions, epsilon)\n",
        "\n",
        "            return predictions, std_predictions\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def get_variable_importance(self, X, labels=None):\n",
        "        \"\"\"\n",
        "        Get variable importance metrics from the fitted model.\n",
        "\n",
        "        Args:\n",
        "            X: Feature matrix\n",
        "            labels: List of feature names\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with variable importance results\n",
        "        \"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise RuntimeError(\"Model must be fitted before computing variable importance\")\n",
        "\n",
        "        # Fix for the labels input based on NumPy docs\n",
        "        if labels is not None:\n",
        "            if isinstance(labels, np.ndarray):\n",
        "                labels = labels.tolist()  # Convert numpy array to list\n",
        "            elif isinstance(labels, list):\n",
        "                # Already a list, keep as is\n",
        "                pass\n",
        "            else:\n",
        "                # Other types, try to convert\n",
        "                labels = list(labels)\n",
        "\n",
        "        # Calculate variable importance\n",
        "        try:\n",
        "            variable_importance = pmb.get_variable_inclusion(self.idata, X, labels=labels)\n",
        "\n",
        "            # Compute more detailed variable importance\n",
        "            vi_results = pmb.compute_variable_importance(\n",
        "                self.idata,\n",
        "                self.model.named_vars[\"μ\"],\n",
        "                X,\n",
        "                method=\"VI\"\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                \"normalized_vi\": variable_importance[0],\n",
        "                \"variable_labels\": variable_importance[1],\n",
        "                \"detailed_results\": vi_results\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error in calculating variable importance: {str(e)}\")\n",
        "            print(f\"Type of labels: {type(labels)}\")\n",
        "            if labels is not None:\n",
        "                print(f\"First few labels: {labels[:5] if len(labels) > 5 else labels}\")\n",
        "            raise\n",
        "\n",
        "    def plot_variable_importance(self, X, labels=None):\n",
        "        \"\"\"Plot variable importance metrics.\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise RuntimeError(\"Model must be fitted before plotting variable importance\")\n",
        "\n",
        "        # Fix for the labels input based on NumPy docs\n",
        "        if labels is not None:\n",
        "            if isinstance(labels, np.ndarray):\n",
        "                labels = labels.tolist()  # Convert numpy array to list\n",
        "            elif isinstance(labels, list):\n",
        "                # Already a list, keep as is\n",
        "                pass\n",
        "            else:\n",
        "                # Other types, try to convert\n",
        "                labels = list(labels)\n",
        "\n",
        "        return pmb.plot_variable_inclusion(self.idata, X, labels=labels)\n",
        "\n",
        "\n",
        "class FinancialPredictionModel:\n",
        "    \"\"\"\n",
        "    Hybrid model combining embedding processing with PyMC-BART for financial forecasting\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim=EMBEDDING_DIM,\n",
        "        output_dim=FUSION_OUTPUT_DIM,\n",
        "        n_trees=50,\n",
        "        response_type=\"continuous\",\n",
        "        num_classes=None\n",
        "    ):\n",
        "        # Feature extractor with consistent embedding dimensions\n",
        "        self.feature_extractor = EmbeddingProcessor(\n",
        "            embedding_dim=embedding_dim,\n",
        "            output_dim=output_dim\n",
        "        )\n",
        "\n",
        "        # PyMC-BART prediction head\n",
        "        self.pymc_bart_head = PyMcBartPredictionHead(\n",
        "            n_trees=n_trees,\n",
        "            response_type=response_type,\n",
        "            num_classes=num_classes\n",
        "        )\n",
        "\n",
        "    def extract_features(self, stock_embeds, sentiment_embeds):\n",
        "        \"\"\"Extract features from input data\"\"\"\n",
        "        with torch.no_grad():\n",
        "            features = self.feature_extractor(\n",
        "                stock_embeds,\n",
        "                sentiment_embeds\n",
        "            )\n",
        "        return features.cpu().numpy()\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        stock_embeds,\n",
        "        sentiment_embeds,\n",
        "        labels,\n",
        "        chains=4,  # Increased for better uncertainty estimates\n",
        "        tune=1000,\n",
        "        draws=1000,\n",
        "        cores=2,  # Use multiple cores on Colab\n",
        "        random_seed=None\n",
        "    ):\n",
        "        \"\"\"Fit the model to training data\"\"\"\n",
        "        # Extract features\n",
        "        X = self.extract_features(\n",
        "            stock_embeds,\n",
        "            sentiment_embeds\n",
        "        )\n",
        "\n",
        "        # Convert labels to numpy if needed\n",
        "        if isinstance(labels, torch.Tensor):\n",
        "            labels = labels.cpu().numpy()\n",
        "\n",
        "        # Fit PyMC-BART head\n",
        "        self.pymc_bart_head.fit(\n",
        "            X,\n",
        "            labels,\n",
        "            chains=chains,\n",
        "            tune=tune,\n",
        "            draws=draws,\n",
        "            cores=cores,\n",
        "            random_seed=random_seed\n",
        "        )\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        stock_embeds,\n",
        "        sentiment_embeds,\n",
        "        return_std=False,\n",
        "        samples=100\n",
        "    ):\n",
        "        \"\"\"Make predictions with uncertainty estimates\"\"\"\n",
        "        # Extract features\n",
        "        X = self.extract_features(\n",
        "            stock_embeds,\n",
        "            sentiment_embeds\n",
        "        )\n",
        "\n",
        "        # Make predictions\n",
        "        return self.pymc_bart_head.predict(X, return_std=return_std, samples=samples)\n",
        "\n",
        "    def get_variable_importance(self, stock_embeds, sentiment_embeds, feature_names=None):\n",
        "        \"\"\"Get variable importance metrics\"\"\"\n",
        "        # Extract features\n",
        "        X = self.extract_features(\n",
        "            stock_embeds,\n",
        "            sentiment_embeds\n",
        "        )\n",
        "\n",
        "        # Fix for the labels input - convert properly for pymc_bart\n",
        "        if feature_names is not None:\n",
        "            if isinstance(feature_names, np.ndarray):\n",
        "                feature_names = feature_names.tolist()\n",
        "            elif isinstance(feature_names, list):\n",
        "                # Keep as is\n",
        "                pass\n",
        "            else:\n",
        "                # Convert other types if necessary\n",
        "                feature_names = list(feature_names)\n",
        "\n",
        "        # Pass to the PyMcBartPredictionHead\n",
        "        return self.pymc_bart_head.get_variable_importance(X, labels=feature_names)\n",
        "\n",
        "    def plot_variable_importance(self, stock_embeds, sentiment_embeds, feature_names=None):\n",
        "        \"\"\"Plot variable importance\"\"\"\n",
        "        X = self.extract_features(\n",
        "            stock_embeds,\n",
        "            sentiment_embeds\n",
        "        )\n",
        "\n",
        "        # Fix for the labels input - convert properly for pymc_bart\n",
        "        if feature_names is not None:\n",
        "            if isinstance(feature_names, np.ndarray):\n",
        "                feature_names = feature_names.tolist()\n",
        "            elif isinstance(feature_names, list):\n",
        "                # Keep as is\n",
        "                pass\n",
        "            else:\n",
        "                # Convert other types if necessary\n",
        "                feature_names = list(feature_names)\n",
        "\n",
        "        return self.pymc_bart_head.plot_variable_importance(X, labels=feature_names)\n",
        "\n",
        "    def save_model(self, path=None):\n",
        "        \"\"\"Save model to disk\"\"\"\n",
        "        if path is None:\n",
        "            path = os.path.join(DRIVE_PATH, \"financial_model_traces.nc\")\n",
        "\n",
        "        if not self.pymc_bart_head.is_fitted:\n",
        "            raise RuntimeError(\"Model must be fitted before saving\")\n",
        "\n",
        "        # Save PyMC-BART traces\n",
        "        self.pymc_bart_head.idata.to_netcdf(path)\n",
        "        print(f\"Model traces saved to {path}\")\n",
        "\n",
        "        # Save PyTorch feature extractor\n",
        "        torch_path = os.path.join(os.path.dirname(path), \"feature_extractor.pt\")\n",
        "        torch.save(self.feature_extractor.state_dict(), torch_path)\n",
        "        print(f\"Feature extractor saved to {torch_path}\")\n",
        "\n",
        "        return path\n",
        "\n",
        "    def load_model(self, path=None, trace_path=None, extractor_path=None):\n",
        "        \"\"\"Load model from disk\"\"\"\n",
        "        if path is not None:\n",
        "            # Use path as base directory\n",
        "            trace_path = path if trace_path is None else trace_path\n",
        "            extractor_path = os.path.join(os.path.dirname(path), \"feature_extractor.pt\") if extractor_path is None else extractor_path\n",
        "        elif trace_path is None:\n",
        "            trace_path = os.path.join(DRIVE_PATH, \"financial_model_traces.nc\")\n",
        "            extractor_path = os.path.join(DRIVE_PATH, \"feature_extractor.pt\")\n",
        "\n",
        "        # Load PyMC-BART traces\n",
        "        try:\n",
        "            self.pymc_bart_head.idata = az.from_netcdf(trace_path)\n",
        "            print(f\"Model traces loaded from {trace_path}\")\n",
        "\n",
        "            # Load PyTorch feature extractor if it exists\n",
        "            if os.path.exists(extractor_path):\n",
        "                self.feature_extractor.load_state_dict(torch.load(extractor_path))\n",
        "                print(f\"Feature extractor loaded from {extractor_path}\")\n",
        "\n",
        "            self.pymc_bart_head.is_fitted = True\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "def load_and_preprocess_data(stock_data_path, sentiment_data_path, max_rows=None):\n",
        "    print(f\"Loading data from: {stock_data_path}, {sentiment_data_path}\")\n",
        "\n",
        "    stock_df = pd.read_csv(stock_data_path)\n",
        "    sentiment_df = pd.read_csv(sentiment_data_path)\n",
        "\n",
        "    min_rows = min(len(stock_df), len(sentiment_df))\n",
        "\n",
        "    if max_rows is not None:\n",
        "        min_rows = min(min_rows, max_rows)\n",
        "\n",
        "    print(f\"Processing {min_rows} rows\")\n",
        "\n",
        "    stock_embeds = _extract_embeddings(stock_df, min_rows)\n",
        "    sentiment_embeds = _extract_sentiment_embeddings(sentiment_df, min_rows)\n",
        "    labels = _extract_labels(stock_df, min_rows)\n",
        "\n",
        "    return {\n",
        "        \"stock_embeds\": stock_embeds,\n",
        "        \"sentiment_embeds\": sentiment_embeds,\n",
        "        \"labels\": labels,\n",
        "        \"num_samples\": min_rows\n",
        "    }\n",
        "\n",
        "def _extract_embeddings(df, rows):\n",
        "    embed_cols = [col for col in df.columns if col.startswith('gpt_embed_')]\n",
        "\n",
        "    if embed_cols:\n",
        "        return torch.tensor(df[embed_cols].values[:rows], dtype=torch.float32)\n",
        "    else:\n",
        "        return torch.randn(rows, EMBEDDING_DIM)\n",
        "\n",
        "def _extract_sentiment_embeddings(df, rows):\n",
        "    sentiment_score_cols = ['sentiment_compound', 'sentiment_positive',\n",
        "                             'sentiment_neutral', 'sentiment_negative']\n",
        "\n",
        "    available_cols = [col for col in sentiment_score_cols if col in df.columns]\n",
        "\n",
        "    if available_cols:\n",
        "        base_features = torch.tensor(df[available_cols].values[:rows], dtype=torch.float32)\n",
        "        projection = nn.Sequential(\n",
        "            nn.Linear(len(available_cols), 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, EMBEDDING_DIM)\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            return projection(base_features)\n",
        "    else:\n",
        "        return torch.randn(rows, EMBEDDING_DIM)\n",
        "\n",
        "def _extract_labels(df, rows):\n",
        "    return torch.tensor(\n",
        "        df['target'].values[:rows] if 'target' in df.columns\n",
        "        else np.random.randn(rows),\n",
        "        dtype=torch.float32\n",
        "    ).unsqueeze(1)\n",
        "\n",
        "def plot_predictions_with_uncertainty(predictions, uncertainties, save_path=None):\n",
        "    \"\"\"\n",
        "    Plot predictions with uncertainty bands\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    x = np.arange(len(predictions))\n",
        "    plt.plot(x, predictions, 'b-', label='Predictions')\n",
        "\n",
        "    # Plot uncertainty bands\n",
        "    plt.fill_between(\n",
        "        x,\n",
        "        predictions - uncertainties,\n",
        "        predictions + uncertainties,\n",
        "        alpha=0.3, color='b', label='Uncertainty'\n",
        "    )\n",
        "\n",
        "    plt.title('Financial Predictions with Uncertainty', fontsize=16)\n",
        "    plt.xlabel('Time', fontsize=12)\n",
        "    plt.ylabel('Value', fontsize=12)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        print(f\"Plot saved to {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Upload datasets to Colab (if not already in Drive)\n",
        "    try:\n",
        "        stock_data_path = os.path.join(DRIVE_PATH, \"stock_data_with_embeddings.csv\")\n",
        "        sentiment_data_path = os.path.join(DRIVE_PATH, \"sentiment_data_with_embeddings.csv\")\n",
        "        # Add after saving each file:\n",
        "        if os.path.exists(file_path):\n",
        "            print(f\"File successfully saved to {file_path} (Size: {os.path.getsize(file_path)} bytes)\")\n",
        "        else:\n",
        "            print(f\"ERROR: Failed to save file to {file_path}\")\n",
        "        # Check if files exist in Drive, if not, ask for upload\n",
        "        if not os.path.exists(stock_data_path) or not os.path.exists(sentiment_data_path):\n",
        "            print(\"Please upload the stock and sentiment data CSV files\")\n",
        "            uploaded = files.upload()\n",
        "            # Add after saving each file:\n",
        "            if os.path.exists(file_path):\n",
        "                print(f\"File successfully saved to {file_path} (Size: {os.path.getsize(file_path)} bytes)\")\n",
        "            else:\n",
        "                print(f\"ERROR: Failed to save file to {file_path}\")\n",
        "\n",
        "            # Move uploaded files to Drive\n",
        "            for filename in uploaded.keys():\n",
        "                if \"stock\" in filename.lower():\n",
        "                    os.replace(filename, stock_data_path)\n",
        "                    print(f\"Moved {filename} to {stock_data_path}\")\n",
        "                elif \"sentiment\" in filename.lower():\n",
        "                    os.replace(filename, sentiment_data_path)\n",
        "                    print(f\"Moved {filename} to {sentiment_data_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error managing file upload: {str(e)}\")\n",
        "        # Fallback paths if there's an issue with Drive\n",
        "        stock_data_path = \"stock_data_with_embeddings.csv\"\n",
        "        sentiment_data_path = \"sentiment_data_with_embeddings.csv\"\n",
        "\n",
        "    # Create and train the model\n",
        "    model = FinancialPredictionModel(\n",
        "        embedding_dim=EMBEDDING_DIM,\n",
        "        output_dim=FUSION_OUTPUT_DIM,\n",
        "        n_trees=50,\n",
        "        response_type=\"continuous\"\n",
        "    )\n",
        "\n",
        "    # Load data with consistent dimensions\n",
        "    data = load_and_preprocess_data(\n",
        "        stock_data_path=stock_data_path,\n",
        "        sentiment_data_path=sentiment_data_path\n",
        "    )\n",
        "\n",
        "    # Extract items from data\n",
        "    stock_embeds = data[\"stock_embeds\"]\n",
        "    sentiment_embeds = data[\"sentiment_embeds\"]\n",
        "    labels = data[\"labels\"]\n",
        "\n",
        "    # Log dimensions to verify consistency\n",
        "    print(f\"Stock embeddings shape: {stock_embeds.shape}\")\n",
        "    print(f\"Sentiment embeddings shape: {sentiment_embeds.shape}\")\n",
        "    print(f\"Labels shape: {labels.shape}\")\n",
        "\n",
        "    # Train the model with multiple chains for better uncertainty estimates\n",
        "    print(\"Fitting the model...\")\n",
        "    model.fit(\n",
        "        stock_embeds=stock_embeds,\n",
        "        sentiment_embeds=sentiment_embeds,\n",
        "        labels=labels.squeeze(),\n",
        "        chains=4,  # Multiple chains for better uncertainty estimates\n",
        "        tune=500,  # More tuning steps\n",
        "        draws=500,  # More draws\n",
        "        cores=2,   # Use multiple cores on Colab\n",
        "        random_seed=123\n",
        "    )\n",
        "\n",
        "    # Make predictions\n",
        "    print(\"Making predictions...\")\n",
        "    predictions, uncertainties = model.predict(\n",
        "        stock_embeds=stock_embeds,\n",
        "        sentiment_embeds=sentiment_embeds,\n",
        "        return_std=True\n",
        "    )\n",
        "\n",
        "    print(f\"Predictions shape: {predictions.shape}\")\n",
        "    print(f\"Uncertainties shape: {uncertainties.shape}\")\n",
        "\n",
        "    # Print uncertainty statistics\n",
        "    print(\"Sample uncertainties:\", uncertainties.flatten()[:10])\n",
        "    print(\"Min uncertainty:\", np.min(uncertainties))\n",
        "    print(\"Max uncertainty:\", np.max(uncertainties))\n",
        "    print(\"Mean uncertainty:\", np.mean(uncertainties))\n",
        "\n",
        "    # Plot predictions with uncertainties\n",
        "    plot_predictions_with_uncertainty(\n",
        "        predictions.flatten(),\n",
        "        uncertainties.flatten(),\n",
        "        save_path=os.path.join(DRIVE_PATH, \"predictions_plot.png\")\n",
        "    )\n",
        "\n",
        "    # Calculate variable importance with better error handling\n",
        "    print(\"Calculating variable importance...\")\n",
        "    feature_names = [f\"Feature_{i}\" for i in range(FUSION_OUTPUT_DIM)]\n",
        "\n",
        "    try:\n",
        "        # First try plotting convergence as recommended in the docs\n",
        "        pmb.plot_convergence(model.pymc_bart_head.idata, var_name=\"μ\")\n",
        "        plt.savefig(os.path.join(DRIVE_PATH, \"convergence_plot.png\"))\n",
        "        plt.show()\n",
        "\n",
        "        # Try safer variable importance calculation\n",
        "        importance = model.get_variable_importance(\n",
        "            stock_embeds=stock_embeds,\n",
        "            sentiment_embeds=sentiment_embeds,\n",
        "            feature_names=feature_names\n",
        "        )\n",
        "        print(\"Variable importance calculation successful!\")\n",
        "\n",
        "        # Try plotting variable importance\n",
        "        try:\n",
        "            vi_results = pmb.compute_variable_importance(\n",
        "                model.pymc_bart_head.idata,\n",
        "                model.pymc_bart_head.model.named_vars[\"μ\"],\n",
        "                model.extract_features(stock_embeds, sentiment_embeds)\n",
        "            )\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            pmb.plot_variable_importance(vi_results)\n",
        "            plt.savefig(os.path.join(DRIVE_PATH, \"variable_importance.png\"))\n",
        "            plt.show()\n",
        "        except Exception as e:\n",
        "            print(f\"Could not plot variable importance: {str(e)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating variable importance: {str(e)}\")\n",
        "        print(\"Variable importance is not crucial for BART models; continuing with predictions\")\n",
        "\n",
        "    # Save the model traces and feature extractor\n",
        "    print(\"Saving model...\")\n",
        "    model.save_model()\n",
        "\n",
        "    # Save predictions to CSV\n",
        "\n",
        "    print(\"Saving predictions to CSV...\")\n",
        "    results_df = pd.DataFrame({\n",
        "        \"predictions\": predictions.flatten(),\n",
        "        \"uncertainties\": uncertainties.flatten()\n",
        "    })\n",
        "# Add after saving each file:\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"File successfully saved to {file_path} (Size: {os.path.getsize(file_path)} bytes)\")\n",
        "    else:\n",
        "        print(f\"ERROR: Failed to save file to {file_path}\")\n",
        "    results_path = os.path.join(DRIVE_PATH, \"model_predictions.csv\")\n",
        "    results_df.to_csv(results_path, index=False)\n",
        "    print(f\"Predictions saved to {results_path}\")\n",
        "\n",
        "    # Add partial dependence plots if features are interpretable\n",
        "    try:\n",
        "        print(\"Generating partial dependence plots...\")\n",
        "        # Get the BART variable directly from the model\n",
        "        bart_var = model.pymc_bart_head.model.named_vars[\"μ\"]\n",
        "        # Generate feature matrix for PDP\n",
        "        X_features = model.extract_features(stock_embeds, sentiment_embeds)\n",
        "        # Create partial dependence plots\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        pmb.plot_pdp(bart_var, X=X_features, Y=labels.squeeze().numpy(),\n",
        "                    grid=(2, 2), func=np.exp)\n",
        "        plt.savefig(os.path.join(DRIVE_PATH, \"partial_dependence_plots.png\"))\n",
        "        plt.show()\n",
        "        print(\"Partial dependence plots generated successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating partial dependence plots: {str(e)}\")\n",
        "\n",
        "    print(\"Training and evaluation complete!\")"
      ]
    },
    {
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "iNR5Ei1cJDxr",
        "outputId": "40eab856-a2ba-455b-e83b-e7a9709d2905"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "partially initialized module 'torch' has no attribute 'fx' (most likely due to a circular import)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-439e2587be43>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2220\u001b[0m \u001b[0;31m# quantization depends on torch.fx and torch.ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2221\u001b[0m \u001b[0;31m# Import quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2222\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquantization\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mquantization\u001b[0m  \u001b[0;31m# usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2224\u001b[0m \u001b[0;31m# Import the quasi random sampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/quantization/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfuser_method_mappings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mobserver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mqconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mquant_type\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mquantization_mappings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/quantization/qconfig.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mhere\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \"\"\"\n\u001b[0;32m----> 9\u001b[0;31m from torch.ao.quantization.qconfig import (\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0m_add_module_to_qconfig_obs_ctr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0m_assert_valid_qconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfuser_method_mappings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mobserver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m from .pt2e._numeric_debugger import (  # noqa: F401\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mcompare_results\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mCUSTOM_KEY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/pt2e/_numeric_debugger.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_sqnr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpt2e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_control_flow_submodules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExportedProgram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraphModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/pt2e/graph_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from torch.fx.passes.utils.source_matcher_utils import (\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mcheck_subgraphs_connected\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mget_source_partitions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/fx/passes/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from . import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mgraph_drawer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgraph_manipulation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnet_min_base\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moperator_support\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/fx/passes/graph_drawer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_format_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_get_qualified_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperator_schemas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalize_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_prop\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorMetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/fx/passes/shape_prop.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcompatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_backward_compatible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mShapeProp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterpreter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \"\"\"\n\u001b[1;32m     87\u001b[0m     \u001b[0mExecute\u001b[0m \u001b[0man\u001b[0m \u001b[0mFX\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mNode\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torch' has no attribute 'fx' (most likely due to a circular import)"
          ]
        }
      ]
    }
  ]
}